{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Ciencia de Datos: Laboratorio 1\n",
    "\n",
    "TODO: Una amazing introduccion sobre la tarea\n",
    "\n",
    "[2]. Link de referenci del obligatorio docentes.\n",
    "\n",
    "[3]. Link a nuestro repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice\n",
    "<a name=\"index\"></a>\n",
    "\n",
    "1. [Imports & Utils](#imports)\n",
    "2. [Adquisición de Datos](#data-adquisition)\n",
    "3. [Entendimiento de los Datos](#data-understanding)\n",
    "    1. [Dominio del Problema](#domain)\n",
    "    2. [EDA: Análisis Exploratorio de Datos](#eda)\n",
    "4. [Procesamiento de los Datos](#data-processing)\n",
    "5. [Análisis de Datos](#data-analysis)\n",
    "6. [Conclusiones](#conclusions)\n",
    "7. [Referencias](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Utils <a name=\"imports\"></a>\n",
    "<a name=\"index\">Volver al Inicio</a>\n",
    "\n",
    "Esta sección contiene todos los imports de dependencias y librerias utilizadas por este proyecto. También contiene la definición de funciones auxiliares utilzadas para obtener los datos y procesarlos. Por último, recuerde instalar los requerimientos (`requirements.txt`) en el mismo entorno donde está ejecutando este notebook y de esa forma evitar errores de import de dependencias (ver [README](README.md))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos algunos parámetros globales del notebook como rutas por defecto y configuraciones similares (para centraliar la configuración de experimentos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals definitions\n",
    "\n",
    "DATA_FOLDER = os.path.join(\n",
    "    \"data\", \"shakespeare\"\n",
    ")  # Path en donde se almacenan los datos de laboratorio 1 en formato\n",
    "DATA_SOURCE = \"local\"  # valid values: local | web\n",
    "SHAKESPEARE_DB_CONN = (\n",
    "    \"mysql+pymysql://guest:relational@db.relational-data.org:3306/Shakespeare\"\n",
    ")\n",
    "\n",
    "DEFAULT_TOP_ROWS_DISPLAY = 10  # Por default cuantas row mostrar con TOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes funciones fueron definidas por el equipo docente y provistas como parte de los recursos del Laboratorio 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definidas por el equipo docente\n",
    "\n",
    "\n",
    "def load_table(data_dir, table_name, engine):\n",
    "    \"\"\"\n",
    "    Leer la tabla con SQL y guardarla como CSV,\n",
    "    o cargarla desde el CSV si ya existe\n",
    "    \"\"\"\n",
    "    path_table = data_dir / f\"{table_name}.csv\"\n",
    "    if not path_table.exists():\n",
    "        print(f\"Consultando tabla con SQL: {table_name}\")\n",
    "        t0 = time()\n",
    "        with engine.connect() as conn:\n",
    "            df_table = pd.read_sql(\n",
    "                sql=f\"SELECT * FROM {table_name}\", con=conn.connection\n",
    "            )\n",
    "        # df_table = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
    "        t1 = time()\n",
    "        print(f\"Tiempo: {t1 - t0:.1f} segundos\")\n",
    "\n",
    "        print(f\"Guardando: {path_table}\\n\")\n",
    "        df_table.to_csv(path_table)\n",
    "    else:\n",
    "        print(f\"Cargando tabla desde CSV: {path_table}\")\n",
    "        df_table = pd.read_csv(path_table, index_col=[0])\n",
    "    return df_table\n",
    "\n",
    "\n",
    "def clean_text(df, column_name):\n",
    "    # Convertir todo a minúsculas\n",
    "    result = df[column_name].str.lower()\n",
    "\n",
    "    # Quitar signos de puntuación y cambiarlos por espacios (\" \")\n",
    "    # TODO: completar signos de puntuación faltantes\n",
    "    for punc in [\"[\", \"\\n\", \",\", \":\", \";\", \".\", \"]\", \"(\", \")\", \"?\", \"!\"]:\n",
    "        result = result.str.replace(punc, \" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes funciones auxiliares fueron definidas por nosotros para facilitar el análisis de datos de este laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definidas por nosotros\n",
    "\n",
    "\n",
    "def read_from_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Método para leer datos desde un archivo CSV local.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta al archivo CSV\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe con datos.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path, sep=\",\", index_col=0)\n",
    "\n",
    "\n",
    "def count_empty_values(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Cuenta valores vacíos.\n",
    "\n",
    "    Esta función cuenta valores vaciós en un dataframe en función del tipo de columna (object, int)\n",
    "    utilizando ciertas convenciones para valores vaciós como que un np.nan y -1 ambos pueden ser valores\n",
    "    válidos para representar un valor faltante o vacío en una columna nunérica.\n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: conteo de vacíos\n",
    "    \"\"\"\n",
    "\n",
    "    def is_empty(column):\n",
    "        if column.dtype == object:  # Assuming object dtype for strings\n",
    "            return column.isin([None, \"\", np.nan])\n",
    "        elif column.dtype == int:\n",
    "            return column.isin([None, np.nan, -1])\n",
    "\n",
    "    empty_counts = df.apply(is_empty).sum()\n",
    "    return empty_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adquisición de los Datos  <a name=\"data-adquisition\"></a>\n",
    "<a name=\"index\">Volver al Inicio</a>\n",
    "\n",
    "Las siguientes celdas se encarga de obtener los datos del Laboratorio 1 y cargarlos en dataframes de pandas para facilitar su análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Descarga de datos.\n",
    "\n",
    "    Este método se encarga de descargar los datos desde el repositorio público de Shakespeare por primera vez,\n",
    "    guardando los datos de cada tabla en un archivo CSV separado.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: Dataframes con los datos de las tablas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creamos el directorio DATA_FOLDER donde se guardarán los CSV\n",
    "    data_dir = Path(DATA_FOLDER)\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Conectando a la base usando url={SHAKESPEARE_DB_CONN}...\")\n",
    "    engine = create_engine(SHAKESPEARE_DB_CONN)\n",
    "\n",
    "    # DataFrame con todas las obras:\n",
    "    df_works = load_table(\"works\", engine)\n",
    "\n",
    "    # Todos los párrafos de todas las obras\n",
    "    df_paragraphs = load_table(\"paragraphs\", engine)\n",
    "\n",
    "    # TODO: cargar el resto de las tablas\n",
    "    # Completamos el código originalmente provisto por los docentes.\n",
    "\n",
    "    # DataFrame con los chapters\n",
    "    df_chapters = load_table(\"chapters\", engine)\n",
    "\n",
    "    # DataFrame con los chapters\n",
    "    df_characters = load_table(\"characters\", engine)\n",
    "\n",
    "    return df_works, df_paragraphs, df_chapters, df_characters\n",
    "\n",
    "\n",
    "def read_local_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Lectura de datos.\n",
    "\n",
    "    Este método se encarga de leer/cargar los datos previamente descargados desde el repositorio público de Shakespeare y guardados localmente. Es útil para experimentos posteriores a la primera vez que se ejecutó este notebook, evitando tener que descargar los datos cada vez que lo ejecutamos.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: Dataframes con los datos de las tablas.\n",
    "    \"\"\"\n",
    "    data_dir = Path(DATA_FOLDER)\n",
    "\n",
    "    # DataFrame con todas las obras:\n",
    "    df_works = read_from_csv(os.path.join(data_dir, \"works.csv\"))\n",
    "\n",
    "    # Todos los párrafos de todas las obras\n",
    "    df_paragraphs = read_from_csv(os.path.join(data_dir, \"paragraphs.csv\"))\n",
    "\n",
    "    # DataFrame con los chapters\n",
    "    df_chapters = read_from_csv(os.path.join(data_dir, \"chapters.csv\"))\n",
    "\n",
    "    # DataFrame con los chapters\n",
    "    df_characters = read_from_csv(os.path.join(data_dir, \"characters.csv\"))\n",
    "\n",
    "    return df_works, df_paragraphs, df_chapters, df_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda cargamos los datos de las tablas: Works, Paragraphs, Chapters y Characters. Más adelante en la siguiente sección entramos en detalles sobre que son los datos de cada una de estas tablas. \n",
    "\n",
    "Por otro lado, los datos se encuentran disponibles en la web en [1] y es posible descargarlos mediante el método provisto por los docentes ```load_table()``` y la librería [SQLAlchemy](https://www.sqlalchemy.org/). Para simplificar este proceso implementamos el método ```download_data()``` que se encarga de articular la descarga utilizando las herramientas mencionadas anteriormente. No obstante, no tiene sentido descargarse desde la web los datos, cada veze que se ejecuta este notebook. Por tal razón los datos se guardan localmente en archivos ```.csv``` en el directorio definido por ```DATA_FOLDER```. Aprovechando eso hemos implementado el método ```read_local_data()``` que articula la lectura de los datos desde archivos ```csv``` en dicho directorio.\n",
    "\n",
    "En la siguiente celda, se cargan los datos en los dataframes de nombre ```df_works, df_paragraphs, df_chapters, df_characters``` utilizando o bien le método ```download_data()``` o ```read_local_data()```, en función de que se encuentre configuradoo en ```DATA_SOURCE```:\n",
    "\n",
    "* DATA_SOURCE='web' -> Utiliza ```download_data()``` para descargar los datos.\n",
    "* DATA_SOURCE='local' -> Utiliza ```read_local_data()``` para leer los datos localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En función de DATA_SOURCE trae los datos de un origien diferente.\n",
    "print(\"Cargando los datos...\")\n",
    "\n",
    "if DATA_SOURCE == \"web\":\n",
    "    df_works, df_paragraphs, df_chapters, df_characters = download_data()\n",
    "elif DATA_SOURCE == \"local\":\n",
    "    df_works, df_paragraphs, df_chapters, df_characters = read_local_data()\n",
    "else:\n",
    "    raise Exception(\n",
    "        \"Debe especificar un tipo de source válido para los datos: 'web' | 'local'.\"\n",
    "    )\n",
    "\n",
    "print(f\"Works: {df_works.shape}\")\n",
    "print(f\"Paragraphs: {df_paragraphs.shape}\")\n",
    "print(f\"Chapters: {df_chapters.shape}\")\n",
    "print(f\"Characters: {df_characters.shape}\")\n",
    "print(\"Datos cargados exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entendimiento de los Datos <a name=\"data-understanding\"></a>\n",
    "<a name=\"index\">Volver al Inicio</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Dominio del Problema  <a name=\"domain\"></a>\n",
    "\n",
    "\n",
    "Más información acerca de las tablas disponibles en la base de datos [aquí](https://relational-data.org/dataset/Shakespeare). \n",
    "\n",
    "![img](assets/image_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. EDA: Análisis Exploratorio de Datos  <a name=\"eda\"></a>\n",
    "\n",
    "Existen muchas técnicas para llevar adelante un análisis exploratorio de datos, algunas dentro de la intuición y otras de una naturaleza más estadísitca. En este trabajo, nos vamos a limitar a ejecutar algunos análisis básicos e intuitivos para principalmente ganar mayor conocimiento sobre los datos analizados y validar algunas hipótesis sobre la calidad de los datos y a su vez nos vamos a apoyar en la libreria [ydata-profiling](https://docs.profiling.ydata.ai/latest/) para realizar automáticamente un análisis más estadístico completo sobre los datos. \n",
    "\n",
    "Para profundizar sobre los objetivos de un análisis exploratorio de datos (EDA) y herramientas disponibles recomendamos la lectura de [4] y [5].\n",
    "\n",
    "**Sobre el Análisis**:\n",
    "\n",
    "Para cada tablas/datarame vamos a conducir principalmente los mismos análisis:\n",
    "\n",
    "1. Vistaso rápido de los datos\n",
    "2. Revisión de Tipos\n",
    "3. Caracterísitcas Macro\n",
    "4. Conteo de missing-values\n",
    "5. Conteo de duplicados\n",
    "6. Revisión de valores inválidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Works\n",
    "\n",
    "Hechemos un vistaso rápido a los datos observando las primeras N filas. Para eso usamos la función [head()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) de pandas. Recordemos que las obras las tenemos cargadas en ```df_works```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra TOP DEFAULT_TOP_ROWS_DISPLAY filas del dataframe\n",
    "df_works.head(DEFAULT_TOP_ROWS_DISPLAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que tipos infirió automáticamente pandas para cada columna, para asegurarnos que son los correctos y revisar si tenemos que hacer algún tipo de procesamiento previo. Para esto accedemos a la propiedad [.dypes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de tipos para las columnas\n",
    "df_works.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos inferir propiedades interesantes del dataframe como los valores mínimo/maximo/avg de cada columna utilizando la función [describe()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) de pandas. Notar que estos valores existen solamente para columnas numéricas y no siempre es relevante su uso. En este caso por ejemplo, nos permite entender más sobre el período en el que se encuentran las obras de Shakespeare analizadas el cual va desde el año 1589 hasta el año 1612. Por otro lado la media de las obras se encuentra hacia el año 1599.\n",
    "\n",
    "Por otro lado, podemos observar que estamos analizando 43 obras. Por último, de acuerdo con Wikipedia [6], Shakespeare nació en el año 1564 y murió en el año 1616 y todos los valores de ```Date``` para works se encuentran dentro de dicho rango. Es un buen indicio de la calidad de los datos en esta columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores descriptivos de cada columna\n",
    "df_works.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer más sencillo el conteo de valores faltantes implementamos el método ```count_empty_values()``` que recibe como parámetro un dataframe y revisa todas las columnas del mismo, contando valores faltantes (missing values). Como sabemos, un valor faltante puede ser tanto un valor ```None``` como un ```0```, ```-1```, ```NaN```, ```''``` (string vacío). Depende del tipo de la columna y el problema principalmente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de missing values por columna\n",
    "empty_values = count_empty_values(df=df_works)\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro problema de calidad puede ser la existencia de valores repetídos, en cuyo caso aveces nos fuerza a tener que eliminarlos previo a realizar un análisis. Para esto vamos a utilizar la función [duplicated()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html) de pandas. Notar que en algunas columnas no solo tenemos valores repetidos sino que tiene mucho sentído y además nos va a dar información relevante sobre la obra de Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos ocurrencias duplicadas en cada columna del dataframe\n",
    "duplicate_counts = {col: df_works[col].duplicated().sum() for col in df_works.columns}\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cuales son los géneros que comprenden a la obra de Shakespeare\n",
    "genres = df_works[\"GenreType\"].unique().tolist()\n",
    "print(\n",
    "    f\"La obra de Shakespeare se concentra en {len(genres)} géneros: {\n",
    "        ', '.join(genres)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No parece haber datos duplicados que requieran de un pre-procesamiento:\n",
    "\n",
    "* ✅ No hay repetidos en columna ```id```\n",
    "* ✅ No hay repetidos en columna ```Title```\n",
    "* ✅ No hay repetidos en columna ```LongTitle```\n",
    "\n",
    "Además:\n",
    "\n",
    "* Hay varios años en los que Shakespeare publicó más de una obra (más adelante analizaremos esto en detalle)\n",
    "* Hay repetición en los géneros lo cual tiene mucho sentido ya que son un grupo de apenas **5** géneros: _Comedy, Tragedy, History, Poem, Sonnet_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores inválidos\n",
    "# NO TIENE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Chapters\n",
    "\n",
    "Repitamos el mismo procedimiento ahora utilizando el dataframe ```df_chapters``` que contiene los datos de capítulos de las obras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra TOP DEFAULT_TOP_ROWS_DISPLAY filas del dataframe\n",
    "df_chapters.head(DEFAULT_TOP_ROWS_DISPLAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de tipos para las columnas\n",
    "df_chapters.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores descriptivos de cada columna\n",
    "df_chapters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de missing values\n",
    "empty_values = count_empty_values(df=df_chapters)\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos ocurrencias duplicadas en cada columna del dataframe\n",
    "duplicate_counts = {\n",
    "    col: df_chapters[col].duplicated().sum() for col in df_chapters.columns\n",
    "}\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, nos interesa además revisar que no existen duplicados en la combinación ```<work_id, Act, Scene, Description>``` que identifica semánticamente a una escena (notar que cada fila está identificada por la columna ```id```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_counts = df_chapters.duplicated(\n",
    "    subset=[\"work_id\", \"Act\", \"Scene\", \"Description\"]\n",
    ").sum()\n",
    "print(f\"Duplicados: {duplicate_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse que todo chapter pertenece a un work válido\n",
    "df_merge = pd.merge(df_chapters, df_works,\n",
    "                    left_on=\"work_id\", right_on=\"id\", how=\"left\")\n",
    "no_match_count = df_merge[\"id_y\"].isna().sum()\n",
    "print(f\"Chapters sin Work: {no_match_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori no hay datos duplicados en capítulos en base a la ausencia de duplicados en la columna ```id``` asi como también en la ausencia de duplicados en las tuplas ```<work_id, Act, Scene, Description>```. \n",
    "\n",
    "* ✅ No hay repetidos en columna ```id```\n",
    "* ✅ No hay repetidos en columna ```Title```\n",
    "* ✅ No hay repetidos en columna ```LongTitle```\n",
    "\n",
    "Por otro lado: \n",
    "* ✅ Todos los capítulos referencian a una obra (work) válida en ```df_works```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Paragraphs\n",
    "\n",
    "Repitamos el mismo procedimiento ahora utilizando el dataframe ```df_paragraphs``` que contiene los datos de párrafos de las obras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra TOP DEFAULT_TOP_ROWS_DISPLAY filas del dataframe\n",
    "df_paragraphs.head(DEFAULT_TOP_ROWS_DISPLAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de tipos para las columnas\n",
    "df_paragraphs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores descriptivos de cada columna\n",
    "df_paragraphs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de missing values\n",
    "empty_values = count_empty_values(df=df_paragraphs)\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos ocurrencias duplicadas en cada columna del dataframe\n",
    "duplicate_counts = {\n",
    "    col: df_paragraphs[col].duplicated().sum() for col in df_paragraphs.columns\n",
    "}\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_counts = df_paragraphs.duplicated(\n",
    "    subset=[\"ParagraphNum\", \"PlainText\", \"character_id\", \"chapter_id\"]\n",
    ").sum()\n",
    "print(f\"Duplicados: {duplicate_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse que todo paragraph pertenece a un chapter válido\n",
    "df_merge = pd.merge(\n",
    "    df_paragraphs, df_chapters, left_on=\"chapter_id\", right_on=\"id\", how=\"left\"\n",
    ")\n",
    "no_match_count = df_merge[\"id_y\"].isna().sum()\n",
    "print(f\"Párrafo con Chapter inexistente: {no_match_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse que todo paragraph referencia a un character válido\n",
    "df_merge = pd.merge(\n",
    "    df_paragraphs, df_characters, left_on=\"character_id\", right_on=\"id\", how=\"left\"\n",
    ")\n",
    "no_match_count = df_merge[\"id_y\"].isna().sum()\n",
    "print(f\"Párrafo con Character inexistente: {no_match_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4. Characters\n",
    "\n",
    "Finalmente, repitamos el mismo procedimiento ahora utilizando el dataframe ```df_characters``` que contiene los datos de personajes de las obras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra TOP DEFAULT_TOP_ROWS_DISPLAY filas del dataframe\n",
    "df_characters.head(DEFAULT_TOP_ROWS_DISPLAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de tipos para las columnas\n",
    "df_characters.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores descriptivos de cada columna\n",
    "df_characters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de missing values\n",
    "empty_values = count_empty_values(df=df_characters)\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos ocurrencias duplicadas en cada columna del dataframe\n",
    "duplicate_counts = {\n",
    "    col: df_characters[col].duplicated().sum() for col in df_characters.columns\n",
    "}\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_counts = df_characters.duplicated(\n",
    "    subset=[\"CharName\", \"Abbrev\"]).sum()\n",
    "print(f\"Duplicados: {duplicate_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_characters[\"count_duplicated\"] = df_characters.groupby([\"CharName\"])[\n",
    "    \"CharName\"\n",
    "].transform(\"size\")\n",
    "df_duplicated = df_characters.drop_duplicates(subset=[\"CharName\", \"Abbrev\"])\n",
    "df_duplicated = df_duplicated.sort_values(\n",
    "    by=\"count_duplicated\", ascending=False)\n",
    "df_duplicated.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Procesamiento de los Datos <a name=\"data-processing\"></a>\n",
    "<a name=\"index\">Volver al Inicio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nueva columna CleanText a partir de PlainText\n",
    "df_paragraphs[\"CleanText\"] = clean_text(df_paragraphs, \"PlainText\")\n",
    "\n",
    "# Veamos la diferencia\n",
    "df_paragraphs[[\"PlainText\", \"CleanText\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte párrafos en listas \"palabra1 palabra2 palabra3\" -> [\"palabra1\", \"palabra2\", \"palabra3\"]\n",
    "df_paragraphs[\"WordList\"] = df_paragraphs[\"CleanText\"].str.split()\n",
    "\n",
    "# Nuevo dataframe: cada fila ya no es un párrafo, sino una sóla palabra\n",
    "df_words = df_paragraphs.explode(\"WordList\")\n",
    "\n",
    "# Quitamos estas columnas redundantes\n",
    "df_words.drop(columns=[\"CleanText\", \"PlainText\"], inplace=True)\n",
    "\n",
    "# Renombramos la columna WordList -> word\n",
    "df_words.rename(columns={\"WordList\": \"word\"}, inplace=True)\n",
    "\n",
    "# Verificar que el número de filas es mucho mayor\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de los Datos <a name=\"data-analysis\"></a>\n",
    "<a name=\"index\">Volver al Inicio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Obras de Shakespeare a través de los años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Conteo de palabras frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = df_paragraphs.explode(\"WordList\")\n",
    "df_words[\"WordList\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import tqdm\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\", \"parser\", \"textcat\"])\n",
    "\n",
    "# New stop words list\n",
    "customize_stop_words = [\"attach\"]\n",
    "\n",
    "# Mark them as stop words\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "\n",
    "words = df_words[\"WordList\"].values\n",
    "clean_words = []\n",
    "for words_batch in tqdm.tqdm(chunks(words, 1000)):\n",
    "    text = \" \".join(words_batch)\n",
    "    clean_words += [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "print(clean_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "# read the mask image\n",
    "# taken from\n",
    "# http://www.stencilry.org/stencils/movies/alice%20in%20wonderland/255fk.jpg\n",
    "alice_mask = np.array(Image.open(path.join(\"data\", \"shakespeare.png\")))\n",
    "\n",
    "\n",
    "words = \" \".join(clean_words)\n",
    "\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=2000,\n",
    "    mask=alice_mask,\n",
    "    contour_width=3,\n",
    "    contour_color=\"steelblue\",\n",
    ")\n",
    "\n",
    "# generate word cloud\n",
    "wc.generate(words)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(path.join(\"data\", \"wordcloud.png\"))\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "# plt.imshow(alice_mask, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Personajes con más cantidad de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos el nombre de los personajes\n",
    "# TODO: des-comentar luego de cargar df_characters\n",
    "df_words = pd.merge(\n",
    "    df_words, df_characters[[\"id\", \"CharName\"]], left_on=\"character_id\", right_on=\"id\"\n",
    ")\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - des-comentar luego de hacer el merge\n",
    "# - Encuentra algún problema en los resultados?\n",
    "\n",
    "words_per_character = (\n",
    "    df_words.groupby(\"CharName_x\")[\"word\"].count().sort_values(ascending=False)\n",
    ")\n",
    "words_per_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: 10 personajes con más palabras\n",
    "char_show = words_per_character[:10]\n",
    "plt.bar(char_show.index, char_show.values)\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Preguntas Adicionales sobre los Datos\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones <a name=\"conclusions\"></a>\n",
    "<a name=\"index\">Volver al Inicio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Referencias <a name=\"references\"></a>\n",
    "<a name=\"index\">Volver al Inicio</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Base de Datos Shakespeare](https://relational-data.org/dataset/Shakespeare)\n",
    "2. [Referencia 1](www.google.com)\n",
    "3. [Referencia 1](www.google.com)\n",
    "4. [Towards Data Science Exploratory Data Analisys](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)\n",
    "5. [Data Camp: Exploratory Data Analysis in Python](https://www.datacamp.com/courses/exploratory-data-analysis-in-python)\n",
    "6. [Wikipedia William Shakespeare](https://en.wikipedia.org/wiki/William_Shakespeare)\n",
    "7. [Stylecloud](https://github.com/minimaxir/stylecloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-cd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
