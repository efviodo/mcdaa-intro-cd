{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"assets/William-Shakespeare.png\" style=\"width: 350px\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Ciencia de Datos: Laboratorio 1\n",
    "\n",
    "William Shakespeare, nació el 23 de abril de 1564 en Stratford-upon-Avon, Inglaterra y en lo que hoy en día parecería una vida corta (52 años), se transformó en una figura titánica del mundo de la literatura. Este dramaturgo y poeta inglés dejó un legado imborrable con sus más de 39 obras literarias, existen al menos dos corrientes que discuten incluso hoy en día la atribución de ciertas obras, entre entre las que se destacan sus tragedias y comedias, obras como \"Hamlet\", \"Romeo y Julieta\" y \"El rey Lear\". Ya sea si has leído alguna obra de William Shakespeare o no, es muy probable que reconozcas algunas frases con origen en su obra como \"Ser o no ser, esa es la cuestión\" o \"El amor es un humo hecho con el vapor de suspiros\". Estas líneas no solo demuestran su maestría lingüística, sino que también reflejan las intrigas universales sobre el amor, el poder y la tragedia, manteniendo su relevancia a través de los siglos.\n",
    "\n",
    "En este trabajo llevado adelante en el contexto del primer Laboratorio [2] del curso Introducción a la Ciencia de Datos de la Facultad de Ingeniería, UdelaR, edición 2024, nos proponemos adentrarnos en la obra de William Shakespeare con un enfoque de ciencia de datos, analizando sus principales obras utilizando algunas técnicas sencillas de análisis de datos.\n",
    "\n",
    "Esperamos que disfrutes este viaje a través de los datos, el tiempo y principalmente, de la lengua inglesa, tanto como nosotros lo hemos disfrutado.\n",
    "\n",
    "\n",
    "_\"Ten más de lo que muestras habla menos de lo que sabes.\"_\n",
    "\n",
    "_William Shakespeare_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice\n",
    "<a name=\"index\"></a>\n",
    "\n",
    "1. [Imports & Utils](#imports)\n",
    "2. [Adquisición de Datos](#data-adquisition)\n",
    "3. [Entendimiento de los Datos](#data-understanding)\n",
    "    1. [Dominio del Problema](#domain)\n",
    "    2. [EDA: Análisis Exploratorio de Datos](#eda)\n",
    "4. [Procesamiento de los Datos](#data-processing)\n",
    "5. [Análisis de Datos](#data-analysis)\n",
    "6. [Conclusiones](#conclusions)\n",
    "7. [Referencias](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Utils <a name=\"imports\"></a>\n",
    "[Volver al Inicio](#index)\n",
    "\n",
    "Esta sección contiene todos los imports de dependencias y librerias utilizadas por este proyecto. También contiene la definición de funciones auxiliares utilzadas para obtener los datos y procesarlos. Por último, recuerde instalar los requerimientos (`requirements.txt`) en el mismo entorno donde está ejecutando este notebook y de esa forma evitar errores de import de dependencias (ver [README](README.md))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import spacy\n",
    "import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from wordcloud import WordCloud\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos algunos parámetros globales del notebook como rutas por defecto y otras configuraciones para centralizar la configuración de experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals definitions\n",
    "\n",
    "DATA_FOLDER = os.path.join(\n",
    "    \"data\", \"shakespeare\"\n",
    ")  # Path en donde se almacenan los datos de laboratorio 1 en formato\n",
    "DATA_SOURCE = \"local\"  # valid values: local | web\n",
    "SHAKESPEARE_DB_CONN = (\n",
    "    \"mysql+pymysql://guest:relational@db.relational-data.org:3306/Shakespeare\"\n",
    ")\n",
    "FIGURES_FOLDER = os.path.join(\"assets\", \"snapshoots\")\n",
    "DATA_REPORTS = os.path.join(\"assets\", \"reports\")\n",
    "DEFAULT_TOP_ROWS_DISPLAY = 10  # Por default cuantas row mostrar con TOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes funciones fueron definidas por el equipo docente y provistas como parte de los recursos del Laboratorio 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definidas por el equipo docente\n",
    "\n",
    "\n",
    "def load_table(data_dir, table_name, engine):\n",
    "    \"\"\"\n",
    "    Leer la tabla con SQL y guardarla como CSV,\n",
    "    o cargarla desde el CSV si ya existe\n",
    "    \"\"\"\n",
    "    path_table = data_dir / f\"{table_name}.csv\"\n",
    "    if not path_table.exists():\n",
    "        print(f\"Consultando tabla con SQL: {table_name}\")\n",
    "        t0 = time()\n",
    "        with engine.connect() as conn:\n",
    "            df_table = pd.read_sql(\n",
    "                sql=f\"SELECT * FROM {table_name}\", con=conn.connection\n",
    "            )\n",
    "        # df_table = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
    "        t1 = time()\n",
    "        print(f\"Tiempo: {t1 - t0:.1f} segundos\")\n",
    "\n",
    "        print(f\"Guardando: {path_table}\\n\")\n",
    "        df_table.to_csv(path_table)\n",
    "    else:\n",
    "        print(f\"Cargando tabla desde CSV: {path_table}\")\n",
    "        df_table = pd.read_csv(path_table, index_col=[0])\n",
    "    return df_table\n",
    "\n",
    "\n",
    "def clean_text(df, column_name):\n",
    "    result = df[column_name].str.lower()  # Convertir todo a minúsculas\n",
    "    result = result.str.strip()  # Remueve espacios en blanco\n",
    "\n",
    "    # Quitar signos de puntuación y cambiarlos por espacios (\" \")\n",
    "    # TODO: completar signos de puntuación faltantes\n",
    "    for punc in [\"[\", \"\\n\", \",\", \":\", \";\", \".\", \"]\", \"(\", \")\", \"?\", \"!\", \"'\", \"-\", \"\\\"\", \"{\", \"}\"]:\n",
    "        result = result.str.replace(punc, \" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes funciones auxiliares fueron definidas por nosotros para facilitar el análisis de datos de este laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definidas por nosotros\n",
    "\n",
    "\n",
    "def read_from_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Método para leer datos desde un archivo CSV local.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta al archivo CSV\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe con datos.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path, sep=\",\", index_col=0)\n",
    "\n",
    "\n",
    "def count_empty_values(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Cuenta valores vacíos.\n",
    "\n",
    "    Esta función cuenta valores vaciós en un dataframe en función del tipo de columna (object, int)\n",
    "    utilizando ciertas convenciones para valores vaciós como que un np.nan y -1 ambos pueden ser valores\n",
    "    válidos para representar un valor faltante o vacío en una columna nunérica.\n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: conteo de vacíos\n",
    "    \"\"\"\n",
    "\n",
    "    def is_empty(column):\n",
    "        if column.dtype == object:  # Assuming object dtype for strings\n",
    "            return column.isin([None, \"\", np.nan])\n",
    "        elif column.dtype == int:\n",
    "            return column.isin([None, np.nan, -1])\n",
    "\n",
    "    empty_counts = df.apply(is_empty).sum()\n",
    "    return empty_counts\n",
    "\n",
    "def generate_pandas_report(df: pd.DataFrame, name: str, path:str) -> None:\n",
    "    \"\"\"Genera EDA report.\n",
    "\n",
    "    Utiliando ydata-profiling genera un reporte exploratorio de los datos recibidos.\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data Frame a analizar.\n",
    "        name (str): Nombre del data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    profile = ProfileReport(df, title=f\"Profiling Report {name}\")\n",
    "    profile.to_file(os.path.join(path, f\"{name}_report.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adquisición de los Datos  <a name=\"data-adquisition\"></a>\n",
    "[Volver al Inicio](#index)\n",
    "\n",
    "Las siguientes celdas se encarga de obtener los datos del Laboratorio 1 y cargarlos en dataframes de pandas para facilitar su análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Descarga de datos.\n",
    "\n",
    "    Este método se encarga de descargar los datos desde el repositorio público de Shakespeare por primera vez,\n",
    "    guardando los datos de cada tabla en un archivo CSV separado.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: Dataframes con los datos de las tablas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creamos el directorio DATA_FOLDER donde se guardarán los CSV\n",
    "    data_dir = Path(DATA_FOLDER)\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Evitamos crear la conexión a la BD si vamos a trabajar local\n",
    "    if DATA_SOURCE == \"web\":\n",
    "        print(f\"Creando conexión a la base usando url={SHAKESPEARE_DB_CONN}...\")\n",
    "        engine = create_engine(SHAKESPEARE_DB_CONN)\n",
    "    elif DATA_SOURCE == \"local\":\n",
    "        print(\"Evitando crear conexión a BD...\")\n",
    "        engine = None\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Debe especificar un tipo de source válido para los datos: 'web' | 'local'.\"\n",
    "        )\n",
    "\n",
    "    # DataFrame con todas las obras:\n",
    "    df_works = load_table(data_dir, \"works\", engine)\n",
    "\n",
    "    # Todos los párrafos de todas las obras\n",
    "    df_paragraphs = load_table(data_dir, \"paragraphs\", engine)\n",
    "\n",
    "    # TODO: cargar el resto de las tablas\n",
    "    # Completamos el código originalmente provisto por los docentes.\n",
    "\n",
    "    # DataFrame con los chapters\n",
    "    df_chapters = load_table(data_dir, \"chapters\", engine)\n",
    "\n",
    "    # DataFrame con los chapters\n",
    "    df_characters = load_table(data_dir, \"characters\", engine)\n",
    "\n",
    "    return df_works, df_paragraphs, df_chapters, df_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda cargamos los datos de las tablas: Works, Paragraphs, Chapters y Characters. Más adelante en la siguiente sección entramos en detalles sobre que son los datos de cada una de estas tablas. \n",
    "\n",
    "Por otro lado, los datos se encuentran disponibles en la web en [1] y es posible descargarlos mediante el método provisto por los docentes ```load_table()``` y la librería [SQLAlchemy](https://www.sqlalchemy.org/). Para simplificar este proceso implementamos el método ```load_dataframes()``` que se encarga de articular la descarga utilizando las herramientas mencionadas anteriormente. No obstante, no tiene sentido descargarse desde la web los datos, cada veze que se ejecuta este notebook. Por tal razón los datos se guardan localmente en archivos ```.csv``` en el directorio definido por ```DATA_FOLDER```. Notar que el método ```load_table()``` prevee esto mismo en caso de encontrar en el directorio destino un archivo .csv con el nombre de la tabla. Por otro lado, evitamos crear la conexión a la BD en caso que se quiera trabajar local, para evitar por ejemplo errores de conexión con la BD. Para esto usamos la variable DATA_SOURCE que nos indica el modo de trabajo:\n",
    "\n",
    "* DATA_SOURCE='web' -> Crea conexión a la BD y utiliza ```load_dataframes()``` para descargar los datos.\n",
    "* DATA_SOURCE='local' -> No crea la conexión a la BD y utiliza ```load_dataframes()``` para leer los datos localmente ya que ```load_table()``` encontrará los archivos .csv.\n",
    "\n",
    "En la siguiente celda, se cargan los datos en los dataframes de nombre ```df_works, df_paragraphs, df_chapters, df_characters```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cargando los datos...\")\n",
    "df_works, df_paragraphs, df_chapters, df_characters = load_dataframes()\n",
    "\n",
    "print(f\"Works: {df_works.shape}\")\n",
    "print(f\"Paragraphs: {df_paragraphs.shape}\")\n",
    "print(f\"Chapters: {df_chapters.shape}\")\n",
    "print(f\"Characters: {df_characters.shape}\")\n",
    "print(\"Datos cargados exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entendimiento de los Datos <a name=\"data-understanding\"></a>\n",
    "[Volver al Inicio](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Dominio del Problema  <a name=\"domain\"></a>\n",
    "\n",
    "\n",
    "Más información acerca de las tablas disponibles en la base de datos [aquí](https://relational-data.org/dataset/Shakespeare). \n",
    "\n",
    "![img](assets/image_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. EDA: Análisis Exploratorio de Datos  <a name=\"eda\"></a>\n",
    "\n",
    "Existen muchas técnicas para llevar adelante un análisis exploratorio de datos, algunas dentro de la intuición y otras de una naturaleza más estadísitca. En este trabajo, nos vamos a limitar a ejecutar algunos análisis básicos e intuitivos para principalmente ganar mayor conocimiento sobre los datos analizados y validar algunas hipótesis sobre la calidad de los datos y a su vez nos vamos a apoyar en la libreria [ydata-profiling](https://docs.profiling.ydata.ai/latest/) para realizar automáticamente un análisis más estadístico completo sobre los datos. \n",
    "\n",
    "Para profundizar sobre los objetivos de un análisis exploratorio de datos (EDA) y herramientas disponibles recomendamos la lectura de [4] y [5].\n",
    "\n",
    "**Sobre el Análisis**:\n",
    "\n",
    "Para cada tablas/datarame vamos a conducir principalmente los mismos análisis:\n",
    "\n",
    "1. Vistaso rápido de los datos\n",
    "2. Revisión de Tipos\n",
    "3. Caracterísitcas Macro\n",
    "4. Conteo de missing-values\n",
    "5. Conteo de duplicados\n",
    "6. Revisión de valores inválidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Works\n",
    "\n",
    "Hechemos un vistaso rápido a los datos observando las primeras N filas. Para eso usamos la función [head()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) de pandas. Recordemos que las obras las tenemos cargadas en ```df_works```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra TOP DEFAULT_TOP_ROWS_DISPLAY filas del dataframe\n",
    "df_works.head(DEFAULT_TOP_ROWS_DISPLAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que tipos infirió automáticamente pandas para cada columna, para asegurarnos que son los correctos y revisar si tenemos que hacer algún tipo de procesamiento previo. Para esto accedemos a la propiedad [.dypes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de tipos para las columnas\n",
    "df_works.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos inferir propiedades interesantes del dataframe como los valores mínimo/maximo/avg de cada columna utilizando la función [describe()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) de pandas. Notar que estos valores existen solamente para columnas numéricas y no siempre es relevante su uso. En este caso por ejemplo, nos permite entender más sobre el período en el que se encuentran las obras de Shakespeare analizadas el cual va desde el año 1589 hasta el año 1612. Por otro lado la media de las obras se encuentra hacia el año 1599.\n",
    "\n",
    "Por otro lado, podemos observar que estamos analizando 43 obras. Por último, de acuerdo con Wikipedia [6], Shakespeare nació en el año 1564 y murió en el año 1616 y todos los valores de ```Date``` para works se encuentran dentro de dicho rango. Es un buen indicio de la calidad de los datos en esta columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores descriptivos de cada columna\n",
    "df_works.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer más sencillo el conteo de valores faltantes implementamos el método ```count_empty_values()``` que recibe como parámetro un dataframe y revisa todas las columnas del mismo, contando valores faltantes (missing values). Como sabemos, un valor faltante puede ser tanto un valor ```None``` como un ```0```, ```-1```, ```NaN```, ```''``` (string vacío). Depende del tipo de la columna y el problema principalmente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de missing values por columna\n",
    "empty_values = count_empty_values(df=df_works)\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro problema de calidad puede ser la existencia de valores repetídos, en cuyo caso aveces nos fuerza a tener que eliminarlos previo a realizar un análisis. Para esto vamos a utilizar la función [duplicated()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html) de pandas. Notar que en algunas columnas no solo tenemos valores repetidos sino que tiene mucho sentído y además nos va a dar información relevante sobre la obra de Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos ocurrencias duplicadas en cada columna del dataframe\n",
    "duplicate_counts = {col: df_works[col].duplicated().sum() for col in df_works.columns}\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cuales son los géneros que comprenden a la obra de Shakespeare\n",
    "genres = df_works[\"GenreType\"].unique().tolist()\n",
    "print(\n",
    "    f\"La obra de Shakespeare se concentra en {len(genres)} géneros: {\n",
    "        ', '.join(genres)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No parece haber datos duplicados que requieran de un pre-procesamiento:\n",
    "\n",
    "* ✅ No hay repetidos en columna ```id```\n",
    "* ✅ No hay repetidos en columna ```Title```\n",
    "* ✅ No hay repetidos en columna ```LongTitle```\n",
    "\n",
    "Además:\n",
    "\n",
    "* Hay varios años en los que Shakespeare publicó más de una obra (más adelante analizaremos esto en detalle)\n",
    "* Hay repetición en los géneros lo cual tiene mucho sentido ya que son un grupo de apenas **5** géneros: _Comedy, Tragedy, History, Poem, Sonnet_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores inválidos\n",
    "# NO TIENE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera ydata profiling report\n",
    "generate_pandas_report(df_works, \"works\", DATA_REPORTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Chapters\n",
    "\n",
    "Repitamos el mismo procedimiento ahora utilizando el dataframe ```df_chapters``` que contiene los datos de capítulos de las obras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra TOP DEFAULT_TOP_ROWS_DISPLAY filas del dataframe\n",
    "df_chapters.head(DEFAULT_TOP_ROWS_DISPLAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de tipos para las columnas\n",
    "df_chapters.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores descriptivos de cada columna\n",
    "df_chapters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de missing values\n",
    "empty_values = count_empty_values(df=df_chapters)\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos ocurrencias duplicadas en cada columna del dataframe\n",
    "duplicate_counts = {\n",
    "    col: df_chapters[col].duplicated().sum() for col in df_chapters.columns\n",
    "}\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, nos interesa además revisar que no existen duplicados en la combinación ```<work_id, Act, Scene, Description>``` que identifica semánticamente a una escena (notar que cada fila está identificada por la columna ```id```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_counts = df_chapters.duplicated(\n",
    "    subset=[\"work_id\", \"Act\", \"Scene\", \"Description\"]\n",
    ").sum()\n",
    "print(f\"Duplicados: {duplicate_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse que todo chapter pertenece a un work válido\n",
    "df_merge = pd.merge(df_chapters, df_works,\n",
    "                    left_on=\"work_id\", right_on=\"id\", how=\"left\")\n",
    "no_match_count = df_merge[\"id_y\"].isna().sum()\n",
    "print(f\"Chapters sin Work: {no_match_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera ydata profiling report\n",
    "generate_pandas_report(df_chapters, \"chapters\", DATA_REPORTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori no hay datos duplicados en capítulos en base a la ausencia de duplicados en la columna ```id``` asi como también en la ausencia de duplicados en las tuplas ```<work_id, Act, Scene, Description>```. \n",
    "\n",
    "* ✅ No hay repetidos en columna ```id```\n",
    "* ✅ No hay repetidos en columna ```Title```\n",
    "* ✅ No hay repetidos en columna ```LongTitle```\n",
    "\n",
    "Por otro lado: \n",
    "* ✅ Todos los capítulos referencian a una obra (work) válida en ```df_works```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Paragraphs\n",
    "\n",
    "Repitamos el mismo procedimiento ahora utilizando el dataframe ```df_paragraphs``` que contiene los datos de párrafos de las obras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra TOP DEFAULT_TOP_ROWS_DISPLAY filas del dataframe\n",
    "df_paragraphs.head(DEFAULT_TOP_ROWS_DISPLAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de tipos para las columnas\n",
    "df_paragraphs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores descriptivos de cada columna\n",
    "df_paragraphs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de missing values\n",
    "empty_values = count_empty_values(df=df_paragraphs)\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos ocurrencias duplicadas en cada columna del dataframe\n",
    "duplicate_counts = {\n",
    "    col: df_paragraphs[col].duplicated().sum() for col in df_paragraphs.columns\n",
    "}\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_counts = df_paragraphs.duplicated(\n",
    "    subset=[\"ParagraphNum\", \"PlainText\", \"character_id\", \"chapter_id\"]\n",
    ").sum()\n",
    "print(f\"Duplicados: {duplicate_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse que todo paragraph pertenece a un chapter válido\n",
    "df_merge = pd.merge(\n",
    "    df_paragraphs, df_chapters, left_on=\"chapter_id\", right_on=\"id\", how=\"left\"\n",
    ")\n",
    "no_match_count = df_merge[\"id_y\"].isna().sum()\n",
    "print(f\"Párrafo con Chapter inexistente: {no_match_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse que todo paragraph referencia a un character válido\n",
    "df_merge = pd.merge(\n",
    "    df_paragraphs, df_characters, left_on=\"character_id\", right_on=\"id\", how=\"left\"\n",
    ")\n",
    "no_match_count = df_merge[\"id_y\"].isna().sum()\n",
    "print(f\"Párrafo con Character inexistente: {no_match_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisemos si existen párrafos asociados a más de un personaje lo cual no parecería muy lógico ya que en obras de teatro los párrafos son principalmente diálogos y los diálogos son de un solo personaje.\n",
    "\n",
    "Como vemos en la consultra siguiente, no vemos duplicados a nivel de chapter_id, ParagraphNum, PlainText por lo que podemos concluir que cada combinación esta asociada a un único character_id.\n",
    "\n",
    "**SPOILER ALERT**: Si hay diálogos que son repetidos por varios personajes en obras de teatro de Shakespeare pero a nivel de datos están registrados a nombre de un personaje muy peculiar. Seguir leyendo para descrubrirlo!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_paragraphs.groupby(['chapter_id', 'ParagraphNum', 'PlainText']).size().reset_index(name='count')\n",
    "sorted_df = grouped.sort_values(by='count', ascending=False)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_paragraphs.groupby(['chapter_id', 'PlainText']).size().reset_index(name='count')\n",
    "sorted_df = grouped.sort_values(by='count', ascending=False)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera ydata profiling report\n",
    "generate_pandas_report(df_paragraphs, \"paragraphs\", DATA_REPORTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4. Characters\n",
    "\n",
    "Finalmente, repitamos el mismo procedimiento ahora utilizando el dataframe ```df_characters``` que contiene los datos de personajes de las obras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra TOP DEFAULT_TOP_ROWS_DISPLAY filas del dataframe\n",
    "df_characters.head(DEFAULT_TOP_ROWS_DISPLAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de tipos para las columnas\n",
    "df_characters.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores descriptivos de cada columna\n",
    "df_characters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de missing values\n",
    "empty_values = count_empty_values(df=df_characters)\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos ocurrencias duplicadas en cada columna del dataframe\n",
    "duplicate_counts = {\n",
    "    col: df_characters[col].duplicated().sum() for col in df_characters.columns\n",
    "}\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_counts = df_characters.duplicated(\n",
    "    subset=[\"CharName\", \"Abbrev\"]).sum()\n",
    "print(f\"Duplicados: {duplicate_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_counts = df_characters.duplicated(\n",
    "    subset=[\"id\", \"CharName\", \"Abbrev\"]).sum()\n",
    "print(f\"Duplicados: {duplicate_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver en las celdas anteriores tenemos dos caracterśticas interesantes en relación a los personajes:\n",
    "\n",
    "1. La columna Description presenta una cantidad relevante de valores faltantes (NaN más especificamente)\n",
    "2. Hay varios personajes repetidos que podrían tener un significado especial\n",
    "\n",
    "Analicemos más en detalle esto agrupando por las columnas CharName y Abbrev a ver que podemos encontrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_characters[\"count_duplicated\"] = df_characters.groupby([\"CharName\"])[\n",
    "    \"CharName\"\n",
    "].transform(\"size\")\n",
    "df_duplicated = df_characters.drop_duplicates(subset=[\"CharName\", \"Abbrev\"])\n",
    "df_duplicated = df_duplicated.sort_values(\n",
    "    by=\"count_duplicated\", ascending=False)\n",
    "df_duplicated.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_all = df_paragraphs[(df_paragraphs[\"character_id\"] == 68) | (df_paragraphs[\"character_id\"] == 86)]\n",
    "paragraphs_all_ext = pd.merge(paragraphs_all, df_chapters,\n",
    "                    left_on=\"chapter_id\", right_on=\"id\", how=\"left\")\n",
    "paragraphs_all_ext = pd.merge(paragraphs_all_ext, df_works,\n",
    "                    left_on=\"work_id\", right_on=\"id\", how=\"left\")\n",
    "paragraphs_all_ext[[\"PlainText\", \"character_id\", \"chapter_id\", \"Act\", \"Scene\", \"Description\", \"Title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soy curioso, que descripción tendrá y si aparece repetido Hamlet?\n",
    "df_characters[df_characters[\"CharName\"] == \"Hamlet\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las consultas anterior podemos sacar mejores conclusiones acerca de los personajes repetidos:\n",
    "\n",
    "1. Existen algunos personajes \"especiales\" con un significado que trasiende a una obra, más allá de un personaje conocido y con nombre y apellido como _Hamlet_. Este es el caso por ejemplo de: Servant, Lord, First Soldier, Second Lord. Estos son personajes genéricos que pueden aparecer en cualquier obra y podrían desviar algún tipo de análisis que queramos hacer ya que pueden estar presentes en múltiples obras con mismo o similar nombre, pero diferente id.\n",
    "2. Vemos personajes que se llaman igual escritos de diferente forma como Second Lord con Abbrev en minúsculas y mayúsculas (1003 y 1001)\n",
    "3. Hay dos personajes que se los más repetidos: All (y sus variantes) y Messenger (y sus variantes).\n",
    "\n",
    "En resumen, si nos interesa posteriormente por ejemplo identificar los personajes con más párrafos dentro de una obra podemos contar los párrafos mediante un join entre las tablas Characters y Paragraph. Sin embargo si queremos llevar este análisis a la obra completa de Shakespeare, de repente sería bueno plantearnos: ¿Nos interesaría tratar a todos los _Messenger_ o _First Servant_ como uno mismo o diferentes? \n",
    "\n",
    "En función de la respuesta dependerá el tipo de procesamiento previo necesario a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera ydata profiling report\n",
    "generate_pandas_report(df_characters, \"characters\", DATA_REPORTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Procesamiento de los Datos <a name=\"data-processing\"></a>\n",
    "[Volver al Inicio](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nueva columna CleanText a partir de PlainText\n",
    "df_paragraphs[\"CleanText\"] = clean_text(df_paragraphs, \"PlainText\")\n",
    "\n",
    "# Veamos la diferencia\n",
    "df_paragraphs[[\"PlainText\", \"CleanText\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte párrafos en listas \"palabra1 palabra2 palabra3\" -> [\"palabra1\", \"palabra2\", \"palabra3\"]\n",
    "df_paragraphs[\"WordList\"] = df_paragraphs[\"CleanText\"].str.split()\n",
    "\n",
    "# Nuevo dataframe: cada fila ya no es un párrafo, sino una sóla palabra\n",
    "df_words = df_paragraphs.explode(\"WordList\")\n",
    "\n",
    "# Quitamos estas columnas redundantes\n",
    "df_words.drop(columns=[\"CleanText\", \"PlainText\"], inplace=True)\n",
    "\n",
    "# Renombramos la columna WordList -> word\n",
    "df_words.rename(columns={\"WordList\": \"word\"}, inplace=True)\n",
    "\n",
    "# Verificar que el número de filas es mucho mayor\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de los Datos <a name=\"data-analysis\"></a>\n",
    "[Volver al Inicio](#index)\n",
    "\n",
    "En esta sección realizamos diferentes análisis sobre los datos, persiguiendo los siguientes objetivos (en su mayoría descritos en la letra del Laboratorio):\n",
    "\n",
    "1. Analizar las obras de Shakespeare a través de los años e intentar identificar tendencias\n",
    "2. Analizar frecuencias de palabras en la obra de Shakespeare para obtener una perspectiva diferente\n",
    "3. Analizar los personajes con mayor cantidad de palabras asociadas en la obra de Shakespeare\n",
    "\n",
    "Analisis adicionales propuestos:\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Obras de Shakespeare a través de los años\n",
    "\n",
    "Para analizar las obras de Shakespeare a través de ños años alcanza con agrupar las filas de ```df_works``` por año y apartir de allí empear a analizar los resultados. Por ejemplo podemos ver la cantidad de obras producidas por año, o tomar ventanas de tiempo más grande como 5-10 años y analizar la producción desde allí. También puede ser interesante analizar como se distribuye la producción por género literario y ver como evolucionan estas proporciones a traves de los años.\n",
    "\n",
    "Algunos de estos análisis son realizados a continuación con el objetivo de entender la obra de Shakespeare y detectar alguna posible tendencia. \n",
    "\n",
    "Un enfoque complementario, que tambien abordamos en esta sección es recurrir a la literatura. Cualquier autor tiene períodos en su vida que son fácilmente reconocibles en su obra y son ampliamente estudiados por expertos. En el caso de William Shakespeare de acuerdo a [10] se pueden apreciar cuatro períodos en su obra:\n",
    "\n",
    "* In the Workshop (1589-1593)\n",
    "* In the World (1594-1600)\n",
    "* Out of the Depths (1601-1607)\n",
    "* On the Heights (1608-1612)\n",
    "\n",
    "En esta sección a su vez tomamos en cuenta esas referencias para revisar si hay diferencias notorias en los datos que acompañen la teoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de períodos\n",
    "colors = px.colors.qualitative.Pastel2\n",
    "shakespeare_born = 1564\n",
    "shakespeare_periods = {\n",
    "    1: {\n",
    "        \"name\": \"In the Workshop (1589-1593)\",\n",
    "        \"from\": 1589,\n",
    "        \"to\": 1593,\n",
    "        \"color\": colors[0],\n",
    "    },\n",
    "    2: {\n",
    "        \"name\": \"In the World (1594-1600)\",\n",
    "        \"from\": 1594,\n",
    "        \"to\": 1600,\n",
    "        \"color\": colors[1],\n",
    "    },\n",
    "    3: {\n",
    "        \"name\": \"Out of the Depths (1601-1607)\",\n",
    "        \"from\": 1601,\n",
    "        \"to\": 1607,\n",
    "        \"color\": colors[2],\n",
    "    },\n",
    "    4: {\n",
    "        \"name\": \"On the Heights (1608-1612)\",\n",
    "        \"from\": 1608,\n",
    "        \"to\": 1612,\n",
    "        \"color\": colors[3],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero agrupamos por Date y GenreType\n",
    "works_per_year = df_works.groupby([\"Date\", \"GenreType\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Creo una nueva columna con el Total Works por Date\n",
    "works_per_year[\"Total\"] = works_per_year.sum(axis=1)\n",
    "works_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una primera visualización para ver la cantidad de obras por año y por género, dónde cada bloque indica una obra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_per_year_without_total = works_per_year.drop(columns=[\"Total\"])\n",
    "fig, ax = plt.subplots( figsize=(15, 6))\n",
    "bottom= 0\n",
    "\n",
    "for genre, years in works_per_year_without_total.items():\n",
    "\n",
    "    ax.bar(years.index, label=genre, height=years.values,bottom=bottom)\n",
    "    bottom = bottom + years.values\n",
    "plt.xticks(np.arange(1589, 1613,1))\n",
    "plt.title(\"Obras de Shakespeare por año y género\")\n",
    "plt.ylabel(\"Cantidad de obras\")\n",
    "plt.xlabel(\"Año\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obras de Shakespeare por año\n",
    "fig = px.bar(works_per_year, x=works_per_year.index, y=\"Total\", )\n",
    "fig.update_xaxes(tickmode=\"array\", tickvals=works_per_year.index)\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=1200,\n",
    "    title_text=\"Obras de William Shakespeare (por año)\",\n",
    "    xaxis_title=\"Año\",\n",
    "    yaxis_title=\"Obras Producidas (anual)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Guardar imagen\n",
    "fig.write_image(os.path.join(FIGURES_FOLDER, \"obras_por_año_1.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como queda la anterior visualización cuando marcamos explícitamente los períodos en la vida de Shakespeare, descritos por estudiosos de la obra del poeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(works_per_year, x=works_per_year.index, y=\"Total\")\n",
    "fig.update_xaxes(tickmode=\"array\", tickvals=works_per_year.index)\n",
    "\n",
    "for idx, period in shakespeare_periods.items():\n",
    "    fig.add_vrect(\n",
    "        x0=period[\"from\"] - 0.5,\n",
    "        x1=period[\"to\"] + 0.5,\n",
    "        annotation_text=period[\"name\"],\n",
    "        annotation_position=\"top\",\n",
    "        annotation_font_color=\"blue\",\n",
    "        annotation=dict(font_size=15, font_family=\"Arial\"),\n",
    "        fillcolor=period[\"color\"],\n",
    "        opacity=0.5,\n",
    "        line_width=0,\n",
    "    )\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=1100,\n",
    "    title_text=\"Obras de William Shakespeare (por año)\",\n",
    "    xaxis_title=\"Año\",\n",
    "    yaxis_title=\"Obras Producidas (anual)\",\n",
    ")\n",
    "fig.show()\n",
    "#\n",
    "# Guardar imagen\n",
    "fig.write_image(os.path.join(FIGURES_FOLDER, \"obras_por_año_2.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede resultar un poco más intuitivo para entender la evolución del escritor asi como explicar posibles tendencias, analizar la evolución a lo largo de la vida de Shakespeare, comparando contra su edad en lugar de a través de los años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_works[\"Age\"] = df_works[\"Date\"] - shakespeare_born\n",
    "\n",
    "# Primero agrupamos por Date y GenreType\n",
    "works_per_age = df_works.groupby([\"Age\", \"GenreType\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Creo una nueva columna con el Total Works por Date\n",
    "works_per_age[\"Total\"] = works_per_age.sum(axis=1)\n",
    "\n",
    "fig = px.bar(works_per_age, x=works_per_age.index, y=\"Total\")\n",
    "fig.update_xaxes(tickmode=\"array\", tickvals=works_per_age.index)\n",
    "for idx, period in shakespeare_periods.items():\n",
    "    fig.add_vrect(\n",
    "        x0=period[\"from\"] - shakespeare_born - 0.5,\n",
    "        x1=period[\"to\"] - shakespeare_born + 0.5,\n",
    "        annotation_text=period[\"name\"],\n",
    "        annotation_position=\"top\",\n",
    "        annotation_font_color=\"blue\",\n",
    "        annotation=dict(font_size=15, font_family=\"Arial\"),\n",
    "        fillcolor=period[\"color\"],\n",
    "        opacity=0.5,\n",
    "        line_width=0,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=1100,\n",
    "    title_text=\"Obras de William Shakespeare (por edad)\",\n",
    "    xaxis_title=\"Edad (años)\",\n",
    "    yaxis_title=\"Obras Producidas (anual)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Guardar imagen\n",
    "fig.write_image(os.path.join(FIGURES_FOLDER, \"obras_por_edad_1.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_per_year_cumulative = pd.DataFrame(index=works_per_age.index)\n",
    "for col in works_per_age.columns:\n",
    "    works_per_year_cumulative[col] = works_per_age[col].cumsum()\n",
    "\n",
    "plot = go.Figure()\n",
    "\n",
    "for col in works_per_year_cumulative.columns:\n",
    "    if col != \"Total\":\n",
    "        plot.add_trace(\n",
    "            go.Scatter(\n",
    "                name=col,\n",
    "                x=works_per_year_cumulative.index,\n",
    "                y=works_per_year_cumulative[col],\n",
    "                stackgroup=\"one\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "plot.update_layout(\n",
    "    height=600,\n",
    "    width=1100,\n",
    "    title_text=\"Obras de William Shakespeare\",\n",
    "    xaxis_title=\"Edad (años)\",\n",
    "    yaxis_title=\"Acumulado de Obras\",\n",
    ")\n",
    "plot.show()\n",
    "\n",
    "# Guardar imagen\n",
    "plot.write_image(os.path.join(FIGURES_FOLDER, \"obras_por_edad_2.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = go.Figure()\n",
    "\n",
    "# for col in works_per_year_cumulative.columns:\n",
    "#     if col != \"Total\":\n",
    "#         plot.add_trace(go.Scatter(\n",
    "#             name = col,\n",
    "#             x = works_per_year_cumulative.index,\n",
    "#             y = works_per_year_cumulative[col],\n",
    "#             mode='lines'\n",
    "#         ))\n",
    "# plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# fig = go.Figure()\n",
    "fig = make_subplots(rows=5, cols=1)\n",
    "\n",
    "for ix, col in enumerate(works_per_year_cumulative.columns):\n",
    "    if col != \"Total\":\n",
    "        fig.append_trace(\n",
    "            go.Scatter(\n",
    "                x=works_per_year_cumulative.index,\n",
    "                y=works_per_year_cumulative[col],\n",
    "                name=col,\n",
    "                line_shape=\"hv\",\n",
    "            ),\n",
    "            row=ix + 1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "\n",
    "fig.update_layout(height=800, width=1100, title_text=\"Obras de William Shakespeare\")\n",
    "# fig.update_xaxes(tickmode='array',tickvals=works_per_year.index)\n",
    "\n",
    "for idx, period in shakespeare_periods.items():\n",
    "    fig.add_vrect(\n",
    "        x0=period[\"from\"] - shakespeare_born - 0.5,\n",
    "        x1=period[\"to\"] - shakespeare_born + 0.5,\n",
    "        annotation_text=\"\",\n",
    "        annotation_position=\"top\",\n",
    "        annotation_font_color=\"blue\",\n",
    "        annotation=dict(font_size=15, font_family=\"Arial\"),\n",
    "        fillcolor=period[\"color\"],\n",
    "        opacity=0.5,\n",
    "        line_width=0,\n",
    "    )\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Guardar imagen\n",
    "fig.write_image(os.path.join(FIGURES_FOLDER, \"obras_por_edad_3.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Conteo de palabras frecuentes\n",
    "\n",
    "Mediante el análisis de palabras frecuentes podemos obtener una visión diferente sobre el usuo del vocabulario en las obras de William Shakespeare. Para ello lo que vamos a hacer es calcular las frecuencias de palabras y para ello es necesario primero realizar ciertos pre-procesamientos al texto que nos permitan calcualar estas frecuencias de forma acertada:\n",
    "\n",
    "1. Normalizar las palabras: Llevar a minúsculas (lowercase) y remover espacios innecesarios (strip). Notar que esto ya lo hicimos en ```CleanText```\n",
    "2. Separar el texto en palabras: Separar el texto en palabras o tokens para contar posteriromente la frecuencia. Notar que esto ya lo hicimos en ```WordList```.\n",
    "3. Remover stopwords: Algunas palabras como \"and\" o \"or\" son ampliamente utilizadas en el lenguaje pero no aportan mayor valor a nuestro análizis. De considerarlas seguramente se roben el protagonismo de cualquier análisis. Por ello primero vamos a remover stopwords de la lista de palabras ```WordList```, para quedarnos solamente con las palabras más relevantes del idioma.\n",
    "\n",
    "Notar que el paso 2 lo estamos realizando de forma sencilla mediane la función [split()](https://www.w3schools.com/python/ref_string_split.asp) de strings que por defecto utiliza como separador un whitespace ```\" \"```. \n",
    "\n",
    "```python\n",
    "df_paragraphs[\"WordList\"] = df_paragraphs[\"CleanText\"].str.split()\n",
    "```\n",
    "\n",
    "En un trabajo más profundo, podriamos reemplazar este método de tokenización por métodos más complejos y precisos como el [Tokenizer](https://spacy.io/api/tokenizer) de spacy que contempla otro tipo de separadores naturales en el idioma ingles como ```,?-;:```, etc.\n",
    "\n",
    "Para implementar el punto 3 vamos a utilizar la librería de NLP [spacy](https://spacy.io/) que implementa métodos sencillos para remvoer stopwords de un texto. Además, nos evita la tediosa tarea de conseguir las stopwors para un idioma en particular (en este caso inglés), lista de tokens que puede ser bastante larga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos las stopwords de Spacy para Inglés\n",
    "from spacy.lang.en import stop_words\n",
    "\n",
    "stop_words = stop_words.STOP_WORDS\n",
    "print(\n",
    "    f\"Este modelo contiene {len(stop_words)} stop words. Las primeras 10 son: {list(stop_words)[:15]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación limpiamos las palabras usando el modelo ```en_core_web_sm```. Spacy tiene varios modelos en función del idioma. Para esta tarea vamos a utilizar el modelo más sencillo de ingles. Por más información sobre los modelos de Spacy ver [Trained Models & Pipelines](https://spacy.io/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst: List[Any], n: int):\n",
    "    \"\"\"Funcion auxiliar para generar n-sized chunks desde lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "\n",
    "# Cargamos modelo spacy (modelo liviano para ingles)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\", \"parser\", \"textcat\"])\n",
    "\n",
    "# Custom stop words\n",
    "customize_stop_words = []\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True\n",
    "\n",
    "words = df_words[\"word\"].values\n",
    "\n",
    "# Filtro las words de todo el texto manteniendo las que no son stopwords\n",
    "clean_words = []\n",
    "for words_batch in tqdm.tqdm(chunks(words, 1000)):\n",
    "    text = \" \".join(words_batch)\n",
    "    clean_words += [token.lemma_ for token in nlp(text) if not token.is_stop]\n",
    "\n",
    "# Veamos las primeras\n",
    "print(f\"Primeras 10 palabras limpias: {clean_words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos wordcloud a partir de las frecuencias de todas las obras sin distinción\n",
    "words = \" \".join(clean_words)\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=2000,\n",
    "    # mask=alice_mask,\n",
    "    contour_width=3,\n",
    "    contour_color=\"steelblue\",\n",
    ")\n",
    "\n",
    "# generate word cloud\n",
    "wc.generate(words)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(os.path.join(\"assets\", \"snapshoots\", \"wordcloud.png\"))\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Save image\n",
    "plt.savefig(os.path.join(FIGURES_FOLDER, \"wordcloud.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el WordCloud creado a partir de todas la palabras de la obra y sin los stop_words, aparecen palabras pero que en realidad son letras únicas cómo lo son \"d\", \"s\" y \"o\". Luego de una breve investiagación esto puede deberse a que contracciones utilizadas por Shakespeare que no son utilizadas hoy en día, por ejemplo \"o'er\" siendo esta una contracción de over. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuando con este análisis, se puede realizar la visualización de las palabras mas comunes en forma de wordcloud para cada género. Para poder realizar este análisis es necesario poder vincular las palabras al género que fueron utilizadas, debido a las relaciones que tinene las tablas ese necesario realizar diferentes merges para poder obtener la relación buscada. Cómo se mostró en la sección 3, se debe vincular el párrafo a capítulos para obtener los párrafos vinculados a géneros y luego unir las palabras con su párrafo de procedencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs_with_chapter= pd.merge(df_paragraphs[[\"id\",\"ParagraphNum\", \"chapter_id\"]], df_chapters[[\"id\", \"work_id\"]], left_on=\"chapter_id\", right_on=\"id\")\n",
    "df_paragraphs_with_genre = pd.merge(df_paragraphs_with_chapter, df_works[[\"id\", \"GenreType\"]], left_on=\"work_id\", right_on=\"id\")\n",
    "df_words_with_genre = pd.merge(df_words, df_paragraphs_with_genre[[\"id_x\",\"ParagraphNum\", \"GenreType\"]], left_on=\"id\", right_on=\"id_x\", how=\"left\")\n",
    "df_words_with_genre \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una visualización con el conjunto de wordcloud por género."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "for index, i in enumerate([\"Tragedy\", \"Comedy\" , \"History\", \"Sonnet\",\"Poem\"]):\n",
    "    print(i, index)\n",
    "    words = df_words_with_genre.groupby(\"GenreType\")[\"word\"].value_counts()[i].index\n",
    "    words = \" \".join(words)\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        max_words=2000,\n",
    "        # mask=alice_mask,\n",
    "        contour_width=3,\n",
    "        contour_color=\"steelblue\",\n",
    "    )\n",
    "    wc.generate(words)\n",
    "    plt.subplot(2, 3, index+1)\n",
    "    plt.title(\"WordCloud de \"+i)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(FIGURES_FOLDER, \"wordcloud_by_genre.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta visualización se logran ver algunas tendencias, por ejemplo en POem y Sonnet aparece la palabra \"love \"destacada mientras que en el resto de los géneros no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Personajes con más cantidad de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos el nombre de los personajes\n",
    "# TODO: des-comentar luego de cargar df_characters\n",
    "df_words_char = pd.merge(\n",
    "    df_words, df_characters[[\"id\", \"CharName\"]], left_on=\"character_id\", right_on=\"id\"\n",
    ")\n",
    "df_words_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - des-comentar luego de hacer el merge\n",
    "# - Encuentra algún problema en los resultados?\n",
    "\n",
    "words_per_character = (\n",
    "    df_words_char.groupby(\"CharName\")[\"word\"].count().sort_values(ascending=False)\n",
    ")\n",
    "words_per_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ejemplo: 10 personajes con más palabras\n",
    "plt.figure(figsize=(8, 6))\n",
    "char_show = words_per_character[:10]\n",
    "char_show = char_show.sort_values()\n",
    "plt.barh(char_show.index, char_show.values)\n",
    "_ = plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Personaje\")\n",
    "plt.xlabel(\"Cantidad de palabras\")\n",
    "plt.title(\"Cantidad de palabras por personaje\")\n",
    "plt.savefig(os.path.join(FIGURES_FOLDER, \"words_per_character.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_character_df = pd.DataFrame(words_per_character).reset_index()\n",
    "romeo = words_per_character_df[words_per_character_df['CharName'] == 'Romeo']\n",
    "juliet = words_per_character_df[words_per_character_df['CharName'] == 'Juliet']\n",
    "\n",
    "print(f\"Las posiciones en el ranking para Romeo={romeo.index[0]} con {romeo['word'].sum()} palabras y para Julieta={juliet.index[0]} con {juliet['word'].sum()} palabras.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las celdas anteriors nos permiten identificar algunas cosas bien interesantes:\n",
    "\n",
    "1. En el TOP 3 de personajes con mayor cantidad de palabras asociadas se destacan: Poet y (stage directions)\n",
    "2. Los personajes particualres con mayor cantidad de palabras asociadas son: Henry V, Falstaff, Hamlet y Dule of Gloucester. Lejos quedaron del TOP 10 la famosa pareja de Romeo y Julieta (quienes por cierto manejan una cantidad similar de palabras).\n",
    "\n",
    "Analizando más en detalle estos resultados:\n",
    "\n",
    "1. Poet representa al narrador en los poemas escritos por Shakespeare\n",
    "2. Stage Directions de acuerdo a [shakespearestagedirections.coe.edu](https://shakespearestagedirections.coe.edu/#:~:text=Stage%20directions%20are%20where%20the,or%20directors%20about%20that%20information.) son indicaciones particulares escritas por Shakespeare para facilitar la labor de los actores que interpreten un personaje, así como facilitar al lector visualizar la escena que esta ocurriendo. Por ello es de esperarse la aparición de varias acotaciones de este estilo y en particualr como en el dataset las Stage Directions son asociadas a un personaje \"Stage Directions\" es de esperar ver los resultados anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mejoras**\n",
    "\n",
    "A partir del análisis anterior se nos ocurren diferentes mejoras para extender o complementar el análisis:\n",
    "\n",
    "1. Remover Poet y (stage directions) del TOP ahora que sabemos que son dos categorias especiales de personajes.\n",
    "2. Cambiar la forma en la que contamos las palabras por personaje ya que estamos contando stopwords que quizas no sean de tanto interés y \n",
    "además personajes de obras teatrales y comedias naturalmente tendrán mayor cantidad de palabras ya que son obras más extensas. Por ello \n",
    "proponemos normalizar los valores considerando la cantidad de palabras que tiene la obra en la que aparece el personaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10 personajes con más parrafps retirando Poet y Stage Directions\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "char_show = words_per_character[:12]\n",
    "char_show = char_show[2:] # Sacamos poet y Stage Sirections\n",
    "char_show = char_show.sort_values()\n",
    "plt.barh(char_show.index, char_show.values)\n",
    "_ = plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Personaje\")\n",
    "plt.xlabel(\"Cantidad de palabras\")\n",
    "plt.title(\"Cantidad de palabras por personaje\")\n",
    "plt.savefig(os.path.join(FIGURES_FOLDER, \"words_per_character_modified.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos también analizar la cantidad de párrafos por personaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_characters_with_char = pd.merge(df_paragraphs, df_characters, left_on=\"character_id\", right_on=\"id\", how=\"left\")\n",
    "top_characters_by_paragraph = df_characters_with_char.groupby(\"CharName\")[\"CharName\"].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: 10 personajes con más palabras\n",
    "plt.figure(figsize=(8, 6))\n",
    "char_show = top_characters_by_paragraph[:10]\n",
    "char_show = char_show.sort_values()\n",
    "plt.barh(char_show.index, char_show.values)\n",
    "_ = plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Personaje\")\n",
    "plt.xlabel(\"Cantidad de párrafos\")\n",
    "plt.title(\"Cantidad de párrafos por personaje\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: 10 personajes con más palabras\n",
    "char_show = words_per_character[:12]\n",
    "char_show = char_show[2:] # Sacamos poet y Stage Sirections\n",
    "char_show = char_show.sort_values()\n",
    "plt.barh(char_show.index, char_show.values)\n",
    "_ = plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Personaje\")\n",
    "plt.xlabel(\"Cantidad de párrafos\")\n",
    "plt.title(\"Cantidad de párrafos por personaje\")\n",
    "plt.savefig(os.path.join(FIGURES_FOLDER, \"paragraph_per_character.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Preguntas Adicionales sobre los Datos\n",
    "\n",
    "TODO\n",
    "\n",
    "1. Calcular y comparar TOP N palabras más frecuentes por personaje, obra, género (este si lo estamos haciendo)\n",
    "2. En función de los párrrafos por personaje, caracterizarlos en función del uso del vocabulario utiliando alguna técnica de NLP\n",
    "3. Agregar stopwords propias del idioma inglés de la época de Shakespeare para mejorar el pre-procesamiento\n",
    "\n",
    "Agrupar y contar párrrafos por personaje:\n",
    "- Por toda la obra\n",
    "(adicionales)\n",
    "- Por obra\n",
    "- Con o sin normalizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones <a name=\"conclusions\"></a>\n",
    "[Volver al Inicio](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Referencias <a name=\"references\"></a>\n",
    "[Volver al Inicio](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Base de Datos Shakespeare](https://relational-data.org/dataset/Shakespeare)\n",
    "2. [Laboratorio 1](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd/)\n",
    "3. [Referencia 1](www.google.com)\n",
    "4. [Towards Data Science Exploratory Data Analisys](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)\n",
    "5. [Data Camp: Exploratory Data Analysis in Python](https://www.datacamp.com/courses/exploratory-data-analysis-in-python)\n",
    "6. [Wikipedia William Shakespeare](https://en.wikipedia.org/wiki/William_Shakespeare)\n",
    "7. [Python Wordcloud](https://pypi.org/project/wordcloud/)\n",
    "8. [Plotly Vertical Lines](https://plotly.com/python/horizontal-vertical-shapes/)\n",
    "9. [Plotly Colors](https://plotly.com/python/discrete-color/)\n",
    "10. [Four Periods of Shakespeare's Dramatic and Poetic Career](https://moirabaricollegeonline.co.in/attendence/classnotes/files/1589611082.docx#:~:text=Although%20the%20precise%20date%20of,the%20Fourth%20Period%20from%201608.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
